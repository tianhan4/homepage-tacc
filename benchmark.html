<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Dataset and Benchmarks | Smart City TRS Project</title>
	<meta name="description" content="TRS Smart City Project">
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" crossorigin="anonymous">
	<link href="./style.css" rel="stylesheet">
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300;400&display=swap" rel="stylesheet">
	<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
	<link rel="shortcut icon" href="/images/favicon.ico">
</head>

<body>

<div class="menu-container" role="navigation">
	<div class="menu d-flex">
		<div class="logo p-2">
			<img src="https://turing.ust.hk/images/logo.png" height=50 class="d-inline-block align-top">
		</div>  
		<div class="menu-item p-2"><a href="/index.html">Overview</a></div>
		<div class="menu-item p-2"><a href="/people.html">People</a></div>
		<div class="menu-item p-2"><a href="/projects.html">Projects</a></div>
		<div class="menu-item p-2"><a href="/tacc.html">TACC</a></div>
	</div>
</div>

<div class="container-fluid">
	<div class="section section-header" style="height: 300px; min-height: 300px">
		<div class="row section-titles">
			<div class="col">
				<div class="site-title">TACC Machine Learning Benchmark</div>
				<div class="site-subtitle">Comprehensive, easily reproducible machine learning baselines</div>
			</div>
		</div>
	</div>

	<div class="section section-projects">
		<div class="row content">   
			<section class="col-sm-12"><div class="page-header "><h1 class="title"> </h1></div> <a id="main-content"></a><div class="region region-content"> <section id="block-system-main" class="block block-system clearfix"><div class="ds-1col node node-page view-mode-full clearfix"><div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even"><style>
.chart-container {
max-width: 1280px;
}svg {
width: inherit;
height: inherit;
}
text {
fill: inherit;
}.chart-title {
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
font-size: 24px;
color: #76b900;
margin-top: 45px;
margin-bottom: 10px;
letter-spacing: -.02em;
font-weight: 100;
}.chart-subtitle {
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
font-size: 16px;
color: #1a1a1a;
margin-bottom: 10px;
font-weight: 400;
margin-top: 0px;
}.y-axis-label {
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
font-size: 14px;
fill: #1a1a1a;
font-weight: 500;
}.y .tick text.two-lines {
font-size: 12px;
}.y .tick text.title-subtitle tspan:last-of-type {
font-size: 10px;
}.dark .y-axis-label {
fill: #eeeeee;
}.tick-label {
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
font-size: 11px;
fill: #1a1a1a;
font-weight: 500;
}.dark .tick-label {
fill: #eeeeee;
}.axis-label {
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
font-size: 11px;
text-anchor: middle;
fill: #1a1a1a;
font-weight: 500;
}.dark .axis-label {
fill: #eeeeee;
}.axis-label tspan {
text-anchor: middle;
}.bar-label {
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
text-anchor: end;
fill: #ffffff;
font-size: 11px;
font-weight: 500;
}.dark .bar-label {
fill: #000000;
}.outer-bar-label {
fill: #1a1a1a;
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
font-size: 11px;
font-weight: 500;
}.dark .outer-bar-label {
fill: #ffffff;
}.chart-footnote {
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
margin-top: 10px;
margin-bottom: 45px;
font-size: 12px;
color: #999999;
line-height: 1.25em;
font-weight: 400;
}.dark .chart-footnote {
color: #666666;
}.axis path {
fill: #999999;
stroke: #999999;
stroke-width: 1px;
shape-rendering: crispEdges;
}.dark .axis path {
fill: #666666;
stroke: #666666;
}.axis line {
fill: none;
stroke: #dddddd;
stroke-width: 1px;
shape-rendering: crispEdges;
}.dark .axis line {
stroke: #222222;
}.milestone {
stroke-width: 1;
stroke-dasharray: 3, 3;
stroke-dashoffset: 0;
}.legend {
float: right;
margin-right: 20px;
text-align: right;
}.legend-entry {
display: inline-block;
margin-right: 0;
margin-bottom: 10px;
}.legend-square {
width: 10px;
margin-top: 2px;
height: 10px;
float: left;
margin-right: 5px;
margin-left: 20px;
}.legend-text {
float: left;
margin-right: 0;
font-size: 11px;
font-family: "DINWebPro", Helvetica, Arial, Sans-Serif;
font-weight: 500;
color: #1a1a1a;
}.dark .legend-text {
color: #eee;
}
.green100 {
fill: #76b900;
background-color: #76b900;
}.dark .green100 {
fill: #76b900;
background-color: #76b900;
}.green80 {
fill: #91c733;
background-color: #91c733;
}.dark .green80 {
fill: #5e9400;
background-color: #5e9400;
}.green60 {
fill: #add566;
background-color: #add566;
}.dark .green60 {
fill: #476f00;
background-color: #476f00;
}.green40 {
fill: #c8e399;
background-color: #c8e399;
}.dark .green40 {
fill: #3b5d00;
background-color: #3b5d00;
}.competitor-blue {
fill: #7aaad5;
background-color: #7aaad5;
}.dark .competitor-blue {
fill: #004476;
background-color: #004476;
}@media only screen and (max-width: 650px) {
.mobile-hide {
visibility: hidden;
}
}
</style><p>Deploying AI in real world applications, requires training the networks to convergence at a specified accuracy. This is the best methodology to test AI systems- where they are ready to be deployed in the field, as the networks can then deliver meaningful results (for example, correctly performing image recognition on video streams). erformance on MLPerf 1.0 AI Benchmarks</h4><h3 style="color: #76b900;">MLPerf Training Performance</h3><h4>NVIDIA A100 Performance on MLPerf 1.0 AI Benchmarks</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Network</th><th>Time to Train (mins)</th><th>MLPerf Quality Target</th><th>GPU</th><th>Server</th><th>MLPerf-ID</th><th>Precision</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>ResNet-50 v1.5</td><td>28.77</td><td>75.90% classification</td><td>8x A100</td><td>DGX A100</td><td>1.0-1059</td><td>Mixed</td><td>ImageNet2012</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>4.91</td><td>75.90% classification</td><td>64x A100</td><td>DGX A100</td><td>1.0-1064</td><td>Mixed</td><td>ImageNet2012</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>0.58</td><td>75.90% classification</td><td>1024x A100</td><td>DGX A100</td><td>1.0-1072</td><td>Mixed</td><td>ImageNet2012</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>0.4</td><td>75.90% classification</td><td>2480x A100</td><td>DGX A100</td><td>1.0-1076</td><td>Mixed</td><td>ImageNet2012</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td>SSD</td><td>8.52</td><td>23.0% mAP</td><td>8x A100</td><td>DGX A100</td><td>1.0-1059</td><td>Mixed</td><td>COCO2017</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>1.9</td><td>23.0% mAP</td><td>64x A100</td><td>DGX A100</td><td>1.0-1064</td><td>Mixed</td><td>COCO2017</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>0.48</td><td>23.0% mAP</td><td>1024x A100</td><td>DGX A100</td><td>1.0-1072</td><td>Mixed</td><td>COCO2017</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td>UNet-3D</td><td>29.16</td><td>0.908 Mean DICE score</td><td>8x A100</td><td>DGX A100</td><td>1.0-1059</td><td>Mixed</td><td>KiTS19</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>4.68</td><td>0.908 Mean DICE score</td><td>104x A100</td><td>DGX A100</td><td>1.0-1066</td><td>Mixed</td><td>KiTS19</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>3</td><td>0.908 Mean DICE score</td><td>800x A100</td><td>DGX A100</td><td>1.0-1071</td><td>Mixed</td><td>KiTS19</td><td>A100-SXM4-80GB</td></tr><tr><td>PyTorch</td><td>BERT</td><td>21.69</td><td>0.712 Mask-LM accuracy</td><td>8x A100</td><td>DGX A100</td><td>1.0-1060</td><td>Mixed</td><td>Wikipedia 2020/01/01</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>3.37</td><td>0.712 Mask-LM accuracy</td><td>64x A100</td><td>DGX A100</td><td>1.0-1065</td><td>Mixed</td><td>Wikipedia 2020/01/01</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>0.73</td><td>0.712 Mask-LM accuracy</td><td>1024x A100</td><td>DGX A100</td><td>1.0-1073</td><td>Mixed</td><td>Wikipedia 2020/01/01</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>0.32</td><td>0.712 Mask-LM accuracy</td><td>4096x A100</td><td>DGX A100</td><td>1.0-1077</td><td>Mixed</td><td>Wikipedia 2020/01/01</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td>Mask R-CNN</td><td>50.39</td><td>0.377 Box min AP and 0.339 Mask min AP</td><td>8x A100</td><td>DGX A100</td><td>1.0-1060</td><td>Mixed</td><td>COCO2017</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>15.75</td><td>0.377 Box min AP and 0.339 Mask min AP</td><td>32x A100</td><td>DGX A100</td><td>1.0-1062</td><td>Mixed</td><td>COCO2017</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>3.95</td><td>0.377 Box min AP and 0.339 Mask min AP</td><td>272x A100</td><td>DGX A100</td><td>1.0-1070</td><td>Mixed</td><td>COCO2017</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td>RNN-T</td><td>38.7</td><td>0.058 Word Error Rate</td><td>8x A100</td><td>DGX A100</td><td>1.0-1060</td><td>Mixed</td><td>LibriSpeech</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>4.41</td><td>0.058 Word Error Rate</td><td>128x A100</td><td>DGX A100</td><td>1.0-1068</td><td>Mixed</td><td>LibriSpeech</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>2.75</td><td>0.058 Word Error Rate</td><td>1536x A100</td><td>DGX A100</td><td>1.0-1074</td><td>Mixed</td><td>LibriSpeech</td><td>A100-SXM4-80GB</td></tr><tr><td>TensorFlow</td><td>MiniGo</td><td>269.54</td><td>50% win rate vs. checkpoint</td><td>8x A100</td><td>DGX A100</td><td>1.0-1061</td><td>Mixed</td><td>Go</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>29.32</td><td>50% win rate vs. checkpoint</td><td>256x A100</td><td>DGX A100</td><td>1.0-1069</td><td>Mixed</td><td>Go</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>15.53</td><td>50% win rate vs. checkpoint</td><td>1792x A100</td><td>DGX A100</td><td>1.0-1075</td><td>Mixed</td><td>Go</td><td>A100-SXM4-80GB</td></tr><tr><td>NVIDIA Merlin HugeCTR</td><td>DLRM</td><td>1.96</td><td>0.8025 AUC</td><td>8x A100</td><td>DGX A100</td><td>1.0-1058</td><td>Mixed</td><td>Criteo AI Lab’s Terabyte Click-Through-Rate (CTR)</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>1.05</td><td>0.8025 AUC</td><td>64x A100</td><td>DGX A100</td><td>1.0-1063</td><td>Mixed</td><td>Criteo AI Lab’s Terabyte Click-Through-Rate (CTR)</td><td>A100-SXM4-80GB</td></tr><tr><td></td><td></td><td>0.99</td><td>0.8025 AUC</td><td>112x A100</td><td>DGX A100</td><td>1.0-1067</td><td>Mixed</td><td>Criteo AI Lab’s Terabyte Click-Through-Rate (CTR)</td><td>A100-SXM4-80GB</td></tr></tbody></table></div> </small><hr><h3 style="color: #76b900;">Converged Training Performance of NVIDIA A100, A40, A30, A10, V100 and T4</h3><p class="chart-footnote">Benchmarks are reproducible by following links to NGC scripts</p><div id="cnn-training-perf-a100"></div><h4>A100 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Time to Train (mins)</th><th>Accuracy</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.6.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>40</td><td>75.9 Top 1 Accuracy</td><td>22,008 images/sec</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>408</td><td>ImageNet2012</td><td>A100-SXM4-40GB</td></tr><tr><td>PyTorch</td><td>1.8.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_pytorch">Mask R-CNN</a></td><td>176</td><td>.34 AP Segm</td><td>167 images/sec</td><td>8x A100</td><td>DGX A100</td><td>21.12-py3</td><td>TF32</td><td>8</td><td>COCO 2014</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_pytorch">SSD v1.1</a></td><td>43</td><td>.25 mAP</td><td>3,092 images/sec</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>128</td><td>COCO 2017</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>100</td><td>.56 Training Loss</td><td>308,404 total output mels/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>TF32</td><td>128</td><td>LJSpeech 1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>295</td><td>-5.86 Training Loss</td><td>1,453,539 output samples/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:jasper_for_pytorch">Jasper</a></td><td>3,600</td><td>3.53 dev-clean WER</td><td>603 sequences/sec</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>64</td><td>LibriSpeech</td><td>A100 SXM4-40GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>167</td><td>27.76 BLEU Score</td><td>582,721 words/sec</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>10240</td><td>wmt14-en-de</td><td>A100 SXM4-40GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>216</td><td>.18 Training Loss</td><td>1,040,206 frames/sec</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>32</td><td>LJSpeech 1.1</td><td>A100 SXM4-40GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>20</td><td>24.51 BLEU Score</td><td>913,199 total tokens/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>0.37</td><td>.96 Hit Rate at 10</td><td>154,525,495 samples/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>MovieLens 20M</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>3</td><td>91.31 F1</td><td>926 sequences/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>SQuaD v1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Large</a></td><td>405</td><td>14.03 Perplexity</td><td>203,314 total tokens/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>WikiText-103</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>206</td><td>16.94 Perplexity</td><td>637,779 total tokens/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>WikiText-103</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-Large Pre-Training P1</a></td><td>2,379</td><td>-</td><td>3,231 sequences/sec</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>-</td><td>Wikipedia+BookCorpus</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-Large Pre-Training P2</a></td><td>1,377</td><td>1.34 Final Loss</td><td>630 sequences/sec</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>-</td><td>Wikipedia+BookCorpus</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-Large Pre-Training E2E</a></td><td>3,756</td><td>1.34 Final Loss</td><td>-</td><td>8x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>-</td><td>Wikipedia+BookCorpus</td><td>A100-SXM4-40GB</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>95</td><td>77.01 Top1</td><td>20,478 images/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>192</td><td>79.18 Top1</td><td>10,129 images/sec</td><td>8x A100</td><td>DGX A100</td><td>21.04-py4</td><td>Mixed</td><td>256</td><td>Imagenet2012</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>222</td><td>79.48 Top1</td><td>8,743 images/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>Imagenet2012</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>1</td><td>.99 IoU Threshold</td><td>1,027 images/sec</td><td>8x A100</td><td>DGX A100</td><td>21.03-py3</td><td>Mixed</td><td>2</td><td>DAGM2007</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>6</td><td>.9 Dice Score</td><td>952 images/sec</td><td>8x A100</td><td>DGX A100</td><td>21.03-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>1</td><td>.43 NDCG@100</td><td>1,555,512 users processed/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>TF32</td><td>3072</td><td>MovieLens 20M</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.4</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:wideanddeep_for_tensorflow">Wide and Deep</a></td><td>107</td><td>.68 MAP at 12</td><td>1,111,976 samples/sec</td><td>8x A100</td><td>DGX A100</td><td>20.10-py3</td><td>Mixed</td><td>16384</td><td>Kaggle Outbrain Click Prediction</td><td>A100 SXM4-40GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow">BERT-LARGE</a></td><td>11</td><td>91.18 F1</td><td>841 sequences/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>24</td><td>SQuaD v1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine Tuning</a></td><td>3</td><td>92.71 F1</td><td>2,457 sequences/sec</td><td>8x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>SQuaD v1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>2.2.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:efficientnet_for_tensorflow2">EfficientNet-B4</a></td><td>4,231</td><td>82.81 Top 1</td><td>2,535 images/sec</td><td>8x A100</td><td>DGX A100</td><td>20.08-py3</td><td>Mixed</td><td>160</td><td>ImageNet2012</td><td>A100-SXM-80GB</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec | Server with a hyphen is a pre-production server<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384<br> BERT-Large Pre-Training Sequence Length for Phase 1 = 128 and Phase 2 = 512 | Batch Size for Phase 1 = 65,536 and Phase 2 = 32,768<br> EfficientNet-B4: Mixup = 0.2 | Auto-Augmentation | cuDNN Version = 8.0.5.39 | NCCL Version = 2.7.8</p><div id="cnn-training-perf-a40"></div><h4>A40 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Time to Train (mins)</th><th>Accuracy</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>1</td><td>.96 Hit Rate at 10</td><td>59,667,265 samples/sec</td><td>8x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>MovieLens 20M</td><td>A40</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>7</td><td>91.18 F1</td><td>429 sequences/sec</td><td>8x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>SQuaD v1.1</td><td>A40</td></tr></tbody></table></div> </small><p class="chart-footnote">Server with a hyphen is a pre-production server<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384</p><div id="cnn-training-perf-a30"></div><h4>A30 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Time to Train (mins)</th><th>Accuracy</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>182</td><td>77.34 Top1</td><td>10,739 images/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>192</td><td>ImageNet2012</td><td>A30</td></tr><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>215</td><td>.54 Training Loss</td><td>144,326 total output mels/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>533</td><td>-5.82 Training Loss</td><td>794,511 output samples/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>1,108</td><td>27.58 BLEU Score</td><td>87,584 words/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>2560</td><td>wmt14-en-de</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>81</td><td>24.65 BLEU Score</td><td>219,582 total tokens/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>TF32</td><td>128</td><td>wmt16-en-de</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>1</td><td>.96 Hit Rate at 10</td><td>56,399,904 samples/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>MovieLens 20M</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>10</td><td>91.2 F1</td><td>294 sequences/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>151</td><td>22.16 Perplexity</td><td>219,994 total tokens/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>A30</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>198</td><td>76.78 Top1</td><td>9,798 images/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>1</td><td>.99 IoU Threshold 0.95</td><td>580 images/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>2</td><td>DAGM2007</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>9</td><td>.9 DICE Score</td><td>461 images/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>1</td><td>.43 NDCG@100</td><td>861,797 users processed/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>TF32</td><td>3072</td><td>MovieLens 20M</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>573</td><td>79.83 Top1</td><td>3,399 images/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>A30</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine-Tuning</a></td><td>6</td><td>92.65 F1</td><td>904 sequences/sec</td><td>8x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>SQuaD v1.1</td><td>A30</td></tr></tbody></table></div> </small><p class="chart-footnote">Server with a hyphen is a pre-production server<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384</p><div id="cnn-training-perf-a10"></div><h4>A10 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Time to Train (mins)</th><th>Accuracy</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>242</td><td>77.25 Top1</td><td>8,117 images/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>192</td><td>ImageNet2012</td><td>A10</td></tr><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_pytorch">SE-ResNeXt101</a></td><td>996</td><td>80.24 Top1</td><td>1,953 images/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>112</td><td>Imagenet2012</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>204</td><td>.5 Training Loss</td><td>151,946 total output mels/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>637</td><td>-5.84 Training Loss</td><td>664,022 output samples/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>1,365</td><td>27.8 BLEU Score</td><td>70,844 words/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>2560</td><td>wmt14-en-de</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>177</td><td>.25 Training Loss</td><td>467,464 frames/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>LJSpeech 1.1</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>61</td><td>24.49 BLEU Score</td><td>292,052 total tokens/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>1</td><td>.96 Hit Rate at 10</td><td>47,605,092 samples/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>MovieLens 20M</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>13</td><td>91.3 F1</td><td>236 sequences/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>176</td><td>22.16 Perplexity</td><td>187,731 total tokens/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>A10</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>266</td><td>76.74 Top1</td><td>7,283 images/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>1</td><td>.99 IoU Threshold 0.95</td><td>550 images/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>2</td><td>DAGM2007</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>14</td><td>.9 DICE Score</td><td>324 images/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>1</td><td>.43 NDCG@100</td><td>664,902 users processed/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>TF32</td><td>3072</td><td>MovieLens 20M</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>866</td><td>79.65 Top1</td><td>2,240 images/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>A10</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine-Tuning</a></td><td>6</td><td>92.62 F1</td><td>745 sequences/sec</td><td>8x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>SQuaD v1.1</td><td>A10</td></tr></tbody></table></div> </small><p class="chart-footnote">Server with a hyphen is a pre-production server<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384</p><div id="cnn-training-perf-v100"></div><h4>V100 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Time to Train (mins)</th><th>Accuracy</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>169</td><td>77.37 Top1</td><td>11,740 images/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>V100-SXM3-32GB</td></tr><tr><td>PyTorch</td><td>1.8.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_pytorch">Mask R-CNN</a></td><td>269</td><td>.34 AP Segm</td><td>109 images/sec</td><td>8x V100</td><td>DGX-2</td><td>20.12-py3</td><td>Mixed</td><td>8</td><td>COCO 2014</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>185</td><td>.5 Training Loss</td><td>165,599 total output mels/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>458</td><td>-5.65 Training Loss</td><td>922,820 output samples/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:jasper_for_pytorch">Jasper</a></td><td>6,300</td><td>3.49 dev-clean WER</td><td>312 sequences/sec</td><td>8x V100</td><td>DGX-2</td><td>20.06-py3</td><td>Mixed</td><td>64</td><td>LibriSpeech</td><td>V100 SXM2-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>470</td><td>27.6 BLEU Score</td><td>210,671 words/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>5120</td><td>wmt14-en-de</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>354</td><td>.18 Training Loss</td><td>570,968 frames/sec</td><td>8x V100</td><td>DGX-1</td><td>20.06-py3</td><td>Mixed</td><td>32</td><td>LJSpeech 1.1</td><td>V100 SXM2-16GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>39</td><td>24.38 BLEU Score</td><td>447,832 total tokens/sec</td><td>8x V100</td><td>DGX-1</td><td>20.06-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>V100 SXM2-16GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>1</td><td>.96 Hit Rate at 10</td><td>98,983,601 samples/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>MovieLens 20M</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>8</td><td>91.31 F1</td><td>368 sequences/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>115</td><td>22.05 Perplexity</td><td>286,404 total tokens/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>V100-SXM3-32GB</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>185</td><td>76.92 Top1</td><td>10,476 images/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>416</td><td>79.36 Top1</td><td>4,669 images/sec</td><td>8x V100</td><td>DGX-2</td><td>21.04-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>504</td><td>79.81 Top1</td><td>3,867 images/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>1</td><td>.99 IoU Threshold 0.95</td><td>668 images/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>2</td><td>DAGM2007</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>12</td><td>.84 DICE Score</td><td>473 images/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>1</td><td>.43 NDCG@100</td><td>913,611 users processed/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>3072</td><td>MovieLens 20M</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.4</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:wideanddeep_for_tensorflow">Wide and Deep</a></td><td>185</td><td>.68 MAP at 12</td><td>643,334 samples/sec</td><td>8x V100</td><td>DGX-1</td><td>20.10-py3</td><td>Mixed</td><td>16384</td><td>Kaggle Outbrain Click Prediction</td><td>V100 SXM2-16GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow">BERT-LARGE</a></td><td>18</td><td>91.39 F1</td><td>325 sequences/sec</td><td>8x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>2.2.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine-Tuning</a></td><td>6</td><td>92.72 F1</td><td>1,051 sequences/sec</td><td>8x V100</td><td>DGX-1</td><td>20.07-py3</td><td>Mixed</td><td>16</td><td>SQuaD v1.1</td><td>V100 SXM2-16GB</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec <br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384</p><div id="cnn-training-perf-T4"></div><h3 style="color: #76b900;"></h3><h4>T4 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Time to Train (mins)</th><th>Accuracy</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>492</td><td>77.3 Top1</td><td>3,967 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>192</td><td>ImageNet2012</td><td>NVIDIA T4</td></tr><tr><td>PyTorch</td><td>1.7.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_pytorch">ResNeXt101</a></td><td>1,738</td><td>78.75 Top1</td><td>1,124 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>20.10-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_pytorch">SE-ResNeXt101</a></td><td>1,770</td><td>79.94 Top1</td><td>1,102 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>112</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>245</td><td>.52 Training Loss</td><td>126,056 total output mels/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>1,016</td><td>-5.87 Training Loss</td><td>411,083 output samples/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>2,288</td><td>27.65 BLEU Score</td><td>42,030 words/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>2560</td><td>wmt14-en-de</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.7.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>319</td><td>.21 Training Loss</td><td>281,406 frames/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>20.10-py3</td><td>Mixed</td><td>32</td><td>LJSpeech 1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>103</td><td>24.45 BLEU Score</td><td>168,938 total tokens/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>2</td><td>.96 Hit Rate at 10</td><td>28,185,539 samples/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>MovieLens 20M</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>22</td><td>91.34 F1</td><td>136 sequences/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>320</td><td>22.12 Perplexity</td><td>102,981 total tokens/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>NVIDIA T4</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>563</td><td>76.83 Top1</td><td>3,416 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>2</td><td>.99 IoU Threshold 0.95</td><td>312 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>2</td><td>DAGM2007</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>31</td><td>.89 DICE Score</td><td>158 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>2</td><td>.43 NDCG@100</td><td>389,406 users processed/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>3072</td><td>MovieLens 20M</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.4</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_tensorflow">SSD</a></td><td>112</td><td>.28 mAP</td><td>549 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>20.12-py3</td><td>Mixed</td><td>32</td><td>COCO 2017</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_tensorflow2">Mask R-CNN</a></td><td>492</td><td>.34 AP Segm</td><td>53 samples/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.03-py3</td><td>Mixed</td><td>4</td><td>COCO 2014</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>1,224</td><td>79.38 Top1</td><td>1,575 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.02-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>1,552</td><td>79.53 Top1</td><td>1,243 images/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine-Tuning</a></td><td>9</td><td>92.72 F1</td><td>412 sequences/sec</td><td>8x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>SQuaD v1.1</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384</p> <br></div><div class="tab-pane" id="dl-training-single"><p> Deploying AI in real world applications, requires training the networks to convergence at a specified accuracy. This is the best methodology to test AI systems, and is typically done on multi-accelerator systems (see the ‘Training-Convergence’ tab or read our <a href="https://developer.nvidia.com/blog/updating-ai-product-performance-from-throughput-to-time-to-solution/" target="_blank">blog on convergence</a> for more details) to shorten training-to-convergence times, especially for recurrent monthly container builds.</p><p> Scenarios that are not typically used in real-world training, such as single GPU throughput are illustrated in the table below, and provided for reference as an indication of single chip throughput of the platform.</p><p> NVIDIA’s complete solution stack, from hardware to software, allows data scientists to deliver unprecedented acceleration at every scale. Visit <a href="https://ngc.nvidia.com/catalog/collections" target="_blank">NVIDIA GPU Cloud (NGC)</a> to pull containers and quickly get up and running with deep learning.</p><div id="cnn-training-perf-a100"></div><h3 style="color: #76b900;">Single GPU Training Performance of NVIDIA A100, A40, A30, A10, V100 and T4</h3><p class="chart-footnote">Benchmarks are reproducible by following links to NGC scripts</p><h4>A100 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>-</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>2,751 images/sec</td><td>1x A100</td><td>-</td><td>-</td><td>Mixed</td><td>408</td><td>ImageNet2012</td><td>A100-SXM-80GB</td></tr><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_pytorch">Mask R-CNN</a></td><td>29 images/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>TF32</td><td>8</td><td>COCO 2014</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_pytorch">SSD v1.1</a></td><td>445 images/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>COCO 2017</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>38,901 total output mels/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>TF32</td><td>128</td><td>LJSpeech 1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>202,226 output samples/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:jasper_for_pytorch">Jasper</a></td><td>83 sequences/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>LibriSpeech</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.6.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>82,618 words/sec</td><td>1x A100</td><td>DGX A100</td><td>20.06-py3</td><td>Mixed</td><td>10240</td><td>wmt14-en-de</td><td>A100 SXM4-40GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>180,170 frames/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>LJSpeech 1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>157,886 total tokens/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>37,371,854 samples/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>1048576</td><td>MovieLens 20M</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>122 sequences/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>SQuaD v1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Large</a></td><td>28,503 total tokens/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>WikiText-103</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>83,345 total tokens/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>WikiText-103</td><td>A100-SXM-80GB</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>2,662 images/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>1,312 images/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>Imagenet2012</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>1,135 images/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>Imagenet2012</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>365 images/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>DAGM2007</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>149 images/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>393,529 users processed/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>24576</td><td>MovieLens 20M</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:wideanddeep_for_tensorflow">Wide and Deep</a></td><td>321,808 samples/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>TF32</td><td>131072</td><td>Kaggle Outbrain Click Prediction</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow">BERT-LARGE</a></td><td>117 sequences/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>24</td><td>SQuaD v1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine Tuning</a></td><td>348 sequences/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>SQuaD v1.1</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>-</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:efficientnet_for_tensorflow2">EfficientNet-B4</a></td><td>332 images/sec</td><td>1x A100</td><td>DGX A100</td><td>-</td><td>Mixed</td><td>160</td><td>ImageNet2012</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_tensorflow">NCF</a></td><td>40,189,461 samples/sec</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>Mixed</td><td>1048576</td><td>MovieLens 20M</td><td>A100-SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec | Server with a hyphen indicates a pre-production server<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384<br> EfficientNet-B4: Basic Augmentation | cuDNN Version = 8.0.5.32 | NCCL Version = 2.7.8 | Installation Source = NGC</p><div id="cnn-training-perf-a40"></div><h4>A40 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>20,386,212 samples/sec</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>1048576</td><td>MovieLens 20M</td><td>A40</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>62 sequences/sec</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>SQuaD v1.1</td><td>A40</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow">BERT-LARGE</a></td><td>55 sequences/sec</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>24</td><td>SQuaD v1.1</td><td>A40</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec | Server with a hyphen indicates a pre-production server</p><div id="cnn-training-perf-a30"></div><h4>A30 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>1,254 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>192</td><td>ImageNet2012</td><td>A30</td></tr><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_pytorch">SSD v1.1</a></td><td>226 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>COCO 2017</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>18,541 total output mels/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>119,858 output samples/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>24,662 words/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>2560</td><td>wmt14-en-de</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>100,243 frames/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>LJSpeech 1.1</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>19,634,381 samples/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>1048576</td><td>MovieLens 20M</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>55,112 total tokens/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>TF32</td><td>128</td><td>wmt16-en-de</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>40,981 total tokens/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_pytorch">ResNeXt101</a></td><td>544 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>112</td><td>Imagenet2012</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:jasper_for_pytorch">Jasper</a></td><td>34 sequences/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>LibriSpeech</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Large</a></td><td>12,617 total tokens/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>4</td><td>WikiText-103</td><td>A30</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>50 sequences/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>A30</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>1,339 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>595 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>495 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>109 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>DAGM2007</td><td>A30</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>71 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>198,703 users processed/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>24576</td><td>MovieLens 20M</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:wideanddeep_for_tensorflow">Wide and Deep</a></td><td>232,356 samples/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>Kaggle Outbrain Click Prediction</td><td>A30</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_tensorflow2">Mask R-CNN</a></td><td>21 samples/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>4</td><td>COCO 2014</td><td>A30</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine Tuning</a></td><td>143 sequences/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>SQuaD v1.1</td><td>A30</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_tensorflow">SSD</a></td><td>201 images/sec</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>COCO 2017</td><td>A30</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec | Server with a hyphen indicates a pre-production server</p><div id="cnn-training-perf-a10"></div><h4>A10 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>1,019 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>192</td><td>ImageNet2012</td><td>A10</td></tr><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_pytorch">SSD v1.1</a></td><td>175 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>COCO 2017</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>19,741 total output mels/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>100,432 output samples/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>22,248 words/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>2560</td><td>wmt14-en-de</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>92,906 frames/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>LJSpeech 1.1</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>34,753 total tokens/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>66,783 total tokens/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_pytorch">ResNeXt101</a></td><td>421 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.03-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:se_resnext_for_pytorch">SE-ResNeXt101</a></td><td>327 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.03-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>16,862,317 samples/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>1048576</td><td>MovieLens 20M</td><td>A10</td></tr><tr><td></td><td>1.8.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:jasper_for_pytorch">Jasper</a></td><td>29 sequences/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.02-py3</td><td>Mixed</td><td>32</td><td>LibriSpeech</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Large</a></td><td>10,699 total tokens/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>4</td><td>WikiText-103</td><td>A10</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>41 sequences/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>A10</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>996 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>460 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.03-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>342 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.03-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>96 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>DAGM2007</td><td>A10</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>49 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>174,947 users processed/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>24576</td><td>MovieLens 20M</td><td>A10</td></tr><tr><td></td><td>1.15.4</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:wideanddeep_for_tensorflow">Wide and Deep</a></td><td>249,905 samples/sec</td><td>1x A10</td><td>Supermicro SYS-1029GQ-TRT</td><td>20.11-py3</td><td>Mixed</td><td>131072</td><td>Kaggle Outbrain Click Prediction</td><td>A10</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine Tuning</a></td><td>128 sequences/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>SQuaD v1.1</td><td>A10</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_tensorflow2">Mask R-CNN</a></td><td>18 samples/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>4</td><td>COCO 2014</td><td>A10</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_tensorflow">SSD</a></td><td>181 images/sec</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>COCO 2017</td><td>A10</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec | Server with a hyphen indicates a pre-production server</p><div id="cnn-training-perf-v100"></div><h4>V100 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>1,473 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>V100-SXM3-32GB</td></tr><tr><td>PyTorch</td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_pytorch">ResNeXt101</a></td><td>554 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>112</td><td>Imagenet2012</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_pytorch">SSD v1.1</a></td><td>233 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>COCO 2017</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>24,540 total output mels/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>136,702 output samples/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:jasper_for_pytorch">Jasper</a></td><td>44 sequences/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>LibriSpeech</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>32,218 words/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>5120</td><td>wmt14-en-de</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>123,294 frames/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>LJSpeech 1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.7.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>83,200 total tokens/sec</td><td>1x V100</td><td>DGX-2</td><td>20.09-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>22,193,821 samples/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>1048576</td><td>MovieLens 20M</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>53 sequences/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>44,072 total tokens/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Large</a></td><td>15,360 total tokens/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>WikiText-103</td><td>V100-SXM3-32GB</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>1,393 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>632 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>545 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.2</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>169 images/sec</td><td>1x V100</td><td>DGX-1</td><td>20.06-py3</td><td>Mixed</td><td>16</td><td>DAGM2007</td><td>V100 SXM2-16GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>68 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>222,573 users processed/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>24576</td><td>MovieLens 20M</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:wideanddeep_for_tensorflow">Wide and Deep</a></td><td>301,631 samples/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>131072</td><td>Kaggle Outbrain Click Prediction</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_tensorflow">BERT-LARGE</a></td><td>48 sequences/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine-Tuning</a></td><td>192 sequences/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>SQuaD v1.1</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_tensorflow2">Mask R-CNN</a></td><td>22 samples/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>4</td><td>COCO 2014</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_tensorflow">SSD</a></td><td>222 images/sec</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>COCO 2017</td><td>V100-SXM3-32GB</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384</p><div id="cnn-training-perf-T4"></div><h3 style="color: #76b900;"></h3><h4>T4 Training Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Framework</th><th>Framework Version</th><th>Network</th><th>Throughput</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Batch Size</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>MXNet</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_mxnet">ResNet-50 v1.5</a></td><td>514 images/sec</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>21.03-py3</td><td>Mixed</td><td>64</td><td>ImageNet2012</td><td>NVIDIA T4</td></tr><tr><td>PyTorch</td><td>1.8.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_pytorch">ResNeXt101</a></td><td>209 images/sec</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>21.02-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">Tacotron2</a></td><td>16,884 total output mels/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>104</td><td>LJSpeech 1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:tacotron_2_and_waveglow_for_pytorch">WaveGlow</a></td><td>55,618 output samples/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>LJSpeech 1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformer_for_pytorch">Transformer</a></td><td>10,512 words/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>2560</td><td>wmt14-en-de</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:fastpitch_for_pytorch">FastPitch</a></td><td>41,078 frames/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>64</td><td>LJSpeech 1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:gnmt_v2_for_pytorch">GNMT V2</a></td><td>31,038 total tokens/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>wmt16-en-de</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ncf_for_pytorch">NCF</a></td><td>8,104,090 samples/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>1048576</td><td>MovieLens 20M</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:bert_for_pytorch">BERT-LARGE</a></td><td>19 sequences/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>10</td><td>SQuaD v1.1</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Base</a></td><td>17,119 total tokens/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>WikiText-103</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.8.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:jasper_for_pytorch">Jasper</a></td><td>14 sequences/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>20.12-py3</td><td>Mixed</td><td>32</td><td>LibriSpeech</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.8.0a0</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:se_resnext_for_pytorch">SE-ResNeXt101</a></td><td>163 images/sec</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>21.02-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.9.0a0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:transformerxl_for_pytorch">Transformer-XL Large</a></td><td>5,231 total tokens/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>4</td><td>WikiText-103</td><td>NVIDIA T4</td></tr><tr><td>Tensorflow</td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnet_50_v1_5_for_tensorflow">ResNet-50 v1.5</a></td><td>452 images/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>256</td><td>ImageNet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_industrial_for_tensorflow">U-Net Industrial</a></td><td>47 images/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>DAGM2007</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:unet_medical_for_tensorflow2">U-Net Medical</a></td><td>22 images/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>8</td><td>EM segmentation challenge</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/model-scripts/nvidia:vae_for_tensorflow">VAE-CF</a></td><td>83,310 users processed/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>24576</td><td>MovieLens 20M</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_tensorflow">SSD</a></td><td>97 images/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>32</td><td>COCO 2017</td><td>NVIDIA T4</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:mask_r_cnn_for_tensorflow2">Mask R-CNN</a></td><td>9 samples/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>4</td><td>COCO 2014</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:wideanddeep_for_tensorflow">Wide and Deep</a></td><td>203,867 samples/sec</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>21.03-py3</td><td>Mixed</td><td>131072</td><td>Kaggle Outbrain Click Prediction</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:se_resnext_for_tensorflow">SE-ResNext101</a></td><td>170 images/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>96</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>1.15.5</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:resnext_for_tensorflow">ResNext101</a></td><td>207 images/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>128</td><td>Imagenet2012</td><td>NVIDIA T4</td></tr><tr><td></td><td>2.4.0</td><td><a href="https://ngc.nvidia.com/catalog/resources/nvidia:electra_for_tensorflow2">Electra Fine-Tuning</a></td><td>62 sequences/sec</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>Mixed</td><td>16</td><td>SQuaD v1.1</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote">FastPitch throughput metric frames/sec refers to mel-scale spectrogram frames/sec<br> BERT-Large = BERT-Large Fine Tuning (Squadv1.1) with Sequence Length of 384</p></div><div class="tab-pane" id="dl-inference"> <a name="cnn-inf-perf"></a><div id="deeplearningperformance_inference"></div><p> Real-world AI inferencing demands high throughput and low latencies with maximum efficiency across use cases. An industry leading solution enables customers to quickly deploy AI models into real-world production with the highest performance from data centers to the edge.</p><p> NVIDIA landed top performance spots on all <a href="https://developer.nvidia.com/blog/extending-nvidia-performance-leadership-with-mlperf-inference-1-0-results/" target="_blank">MLPerf™ Inference 1.0 tests</a>, the AI-industry’s leading benchmark. NVIDIA TensorRT™ running on NVIDIA Tensor Core GPUs enable the most efficient <a href="#ngc-inference">deep learning inference performance</a> across multiple application areas and models. This versatility provides wide latitude to data scientists to create the optimal low-latency solution. Visit <a href="https://ngc.nvidia.com/catalog/models?orderBy=modifiedDESC&amp;pageNumber=1&amp;queries=&amp;quickFilter=models&amp;filters=" target="_blank">NVIDIA® GPU Cloud (NGC)</a> to download any of these containers and immediately race into production. The <a href="https://www.nvidia.com/en-us/data-center/resources/inference-technical-overview/" target="_blank">inference whitepaper</a> provides an overview of inference platforms.</p><p> The <a href="#deeplearningperformance_inference_triton">Triton Inference Server</a> is an open source inference serving software which maximizes performance and simplifies the deployment of AI models at scale in production. Triton lets teams deploy trained AI models from multiple model frameworks (TensorFlow, TensorRT, PyTorch, ONNX Runtime, OpenVino, or custom backends). They can deploy from local storage, Google Cloud Platform, Azure Storage, or Amazon S3 on any GPU or CPU based infrastructure (in cloud, data center, or embedded devices). Triton is open source on <a href="https://github.com/triton-inference-server/server" target="_blank">GitHub</a> and available as a docker container on <a href="https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver/" target="_blank">NGC</a>.</p> <br><div id="mlperf-benchmarks-offline"></div><h3 style="color: #76b900;">MLPerf Inference v1.0 Performance Benchmarks</h3><h4>Offline Scenario - Closed Division</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Throughput</th><th>Target Accuracy</th><th>GPU</th><th>Server</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>ResNet-50 v1.5</td><td>105,677 samples/sec</td><td>76.46% Top1</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>ImageNet</td><td>A10</td></tr><tr><td></td><td>141,518 samples/sec</td><td>76.46% Top1</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>ImageNet</td><td>A30</td></tr><tr><td></td><td>5,108 samples/sec</td><td>76.46% Top1</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>38,010 samples/sec</td><td>76.46% Top1</td><td>1x A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>304,876 samples/sec</td><td>76.46% Top1</td><td>8x A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>248,179 samples/sec</td><td>76.46% Top1</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>ImageNet</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>132,926 samples/sec</td><td>76.46% Top1</td><td>4x A100</td><td>DGX-Station-A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td>SSD ResNet-34</td><td>2,496 samples/sec</td><td>0.2	mAP</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>COCO</td><td>A10</td></tr><tr><td></td><td>3,756 samples/sec</td><td>0.2	mAP</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>COCO</td><td>A30</td></tr><tr><td></td><td>134 samples/sec</td><td>0.2	mAP</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>989 samples/sec</td><td>0.2	mAP</td><td>1x A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>7,879 samples/sec</td><td>0.2	mAP</td><td>8x A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>6,586 samples/sec</td><td>0.2	mAP</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>COCO</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>3,370 samples/sec</td><td>0.2	mAP</td><td>4x A100</td><td>DGX-Station-A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td>3D-UNet</td><td>172 samples/sec</td><td>0.853	DICE mean</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>BraTS 2019</td><td>A10</td></tr><tr><td></td><td>237 samples/sec</td><td>0.853	DICE mean</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>BraTS 2019</td><td>A30</td></tr><tr><td></td><td>7 samples/sec</td><td>0.853	DICE mean</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>BraTS 2019</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>61 samples/sec</td><td>0.853	DICE mean</td><td>1x A100</td><td>DGX A100</td><td>BraTS 2019</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>480 samples/sec</td><td>0.853	DICE mean</td><td>8x A100</td><td>DGX A100</td><td>BraTS 2019</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>412 samples/sec</td><td>0.853	DICE mean</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>BraTS 2019</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>214 samples/sec</td><td>0.853	DICE mean</td><td>4x A100</td><td>DGX-Station-A100</td><td>BraTS 2019</td><td>A100 SXM-80GB</td></tr><tr><td>RNN-T</td><td>36,116 samples/sec</td><td>7.45%	WER</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>LibriSpeech</td><td>A10</td></tr><tr><td></td><td>51,690 samples/sec</td><td>7.45%	WER</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>LibriSpeech</td><td>A30</td></tr><tr><td></td><td>1,553 samples/sec</td><td>7.45%	WER</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>14,008 samples/sec</td><td>7.45%	WER</td><td>1x A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>105,677 samples/sec</td><td>7.45%	WER</td><td>8x A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>90,853 samples/sec</td><td>7.45%	WER</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>LibriSpeech</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>48,886 samples/sec</td><td>7.45%	WER</td><td>4x A100</td><td>DGX-Station-A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td>BERT</td><td>8,454 samples/sec</td><td>90.07% f1</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>SQuAD v1.1</td><td>A10</td></tr><tr><td></td><td>13,260 samples/sec</td><td>90.07% f1</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>SQuAD v1.1</td><td>A30</td></tr><tr><td></td><td>492 samples/sec</td><td>90.07% f1</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>3,602 samples/sec</td><td>90.07% f1</td><td>1x A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>28,347 samples/sec</td><td>90.07% f1</td><td>8x A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>22,847 samples/sec</td><td>90.07% f1</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>SQuAD v1.1</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>11,305 samples/sec</td><td>90.07% f1</td><td>4x A100</td><td>DGX-Station-A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td>DLRM</td><td>772,378 samples/sec</td><td>80.25% AUC</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>Criteo 1TB Click Logs</td><td>A10</td></tr><tr><td></td><td>1,067,510 samples/sec</td><td>80.25% AUC</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>Criteo 1TB Click Logs</td><td>A30</td></tr><tr><td></td><td>36,473 samples/sec</td><td>80.25% AUC</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>311,826 samples/sec</td><td>80.25% AUC</td><td>1x A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2,462,300 samples/sec</td><td>80.25% AUC</td><td>8x A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>1,057,550 samples/sec</td><td>80.25% AUC</td><td>4x A100</td><td>DGX-Station-A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr></tbody></table></div> </small><h4>Server Scenario - Closed Division</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Throughput</th><th>Target Accuracy</th><th>MLPerf Server Latency<br>Constraints (ms)</th><th>GPU</th><th>Server</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>ResNet-50 v1.5</td><td>87,984 queries/sec</td><td>76.46%	Top1</td><td>15</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>ImageNet</td><td>A10</td></tr><tr><td></td><td>115,987 queries/sec</td><td>76.46%	Top1</td><td>15</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>ImageNet</td><td>A30</td></tr><tr><td></td><td>3,602 queries/sec</td><td>76.46% Top1</td><td>15</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>30,794 queries/sec</td><td>76.46% Top1</td><td>15</td><td>1x A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>259,994 queries/sec</td><td>76.46% Top1</td><td>15</td><td>8x A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>207,976 queries/sec</td><td>76.46% Top1</td><td>15</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>ImageNet</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>106,988 queries/sec</td><td>76.46% Top1</td><td>15</td><td>4x A100</td><td>DGX-Station-A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td>SSD ResNet-34</td><td>2,000 queries/sec</td><td>0.2	mAP</td><td>100</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>COCO</td><td>A10</td></tr><tr><td></td><td>3,575 queries/sec</td><td>0.2	mAP</td><td>100</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>COCO</td><td>A30</td></tr><tr><td></td><td>100 queries/sec</td><td>0.2	mAP</td><td>100</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>926 queries/sec</td><td>0.2	mAP</td><td>100</td><td>1x A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>7,654 queries/sec</td><td>0.2	mAP</td><td>100</td><td>8x A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>6,162 queries/sec</td><td>0.2	mAP</td><td>100</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>COCO</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>3,081 queries/sec</td><td>0.2	mAP</td><td>100</td><td>4x A100</td><td>DGX-Station-A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td>RNN-T</td><td>22,597 queries/sec</td><td>7.45%	WER</td><td>1,000</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>LibriSpeech</td><td>A10</td></tr><tr><td></td><td>36,991 queries/sec</td><td>7.45%	WER</td><td>1,000</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>LibriSpeech</td><td>A30</td></tr><tr><td></td><td>1,303 queries/sec</td><td>7.45%	WER</td><td>1,000</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>12,751 queries/sec</td><td>7.45%	WER</td><td>1,000</td><td>1x A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>103,986 queries/sec</td><td>7.45%	WER</td><td>1,000</td><td>8x A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>85,985 queries/sec</td><td>7.45%	WER</td><td>1,000</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>LibriSpeech</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>43,389 queries/sec</td><td>7.45%	WER</td><td>1,000</td><td>4x A100</td><td>DGX-Station-A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td>BERT</td><td>7,204 queries/sec</td><td>90.07% f1</td><td>130</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>SQuAD v1.1</td><td>A10</td></tr><tr><td></td><td>11,500 queries/sec</td><td>90.07% f1</td><td>130</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>SQuAD v1.1</td><td>A30</td></tr><tr><td></td><td>381 queries/sec</td><td>90.07% f1</td><td>130</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>3,202 queries/sec</td><td>90.07% f1</td><td>130</td><td>1x A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>25,792 queries/sec</td><td>90.07% f1</td><td>130</td><td>8x A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>20,792 queries/sec</td><td>90.07% f1</td><td>130</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>SQuAD v1.1</td><td>A100-PCIe-40GB</td></tr><tr><td></td><td>10,203 queries/sec</td><td>90.07% f1</td><td>130</td><td>4x A100</td><td>DGX-Station-A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td>DLRM</td><td>680,147 queries/sec</td><td>80.25% AUC</td><td>30</td><td>8x A10</td><td>Supermicro 4029GP-TRT-OTO-28</td><td>Criteo 1TB Click Logs</td><td>A10</td></tr><tr><td></td><td>750,204 queries/sec</td><td>80.25% AUC</td><td>30</td><td>8x A30</td><td>Gigabyte G482-Z54</td><td>Criteo 1TB Click Logs</td><td>A30</td></tr><tr><td></td><td>35,991 queries/sec</td><td>80.25% AUC</td><td>30</td><td>1x1g.10gb A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>286,002 queries/sec</td><td>80.25% AUC</td><td>30</td><td>1x A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2,302,570 queries/sec</td><td>80.25% AUC</td><td>30</td><td>8x A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>942,395 queries/sec</td><td>80.25% AUC</td><td>30</td><td>4x A100</td><td>DGX-Station-A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr></tbody></table></div> </small><h4>Power Efficiency Offline Scenario - Closed Division</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Throughput</th><th>Throughput per Watt</th><th>GPU</th><th>Server</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>ResNet-50 v1.5</td><td>213,599 samples/sec</td><td>97.31 samples/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>ImageNet</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>270,706 samples/sec</td><td>78.27 samples/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>124,529 samples/sec</td><td>98.14 samples/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td>SSD ResNet-34</td><td>5,824 samples/sec</td><td>2.6 samples/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>COCO</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>6,875 samples/sec</td><td>1.96 samples/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>3,110 samples/sec</td><td>2.44 samples/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td>3D-UNet</td><td>372 samples/sec</td><td>0.16 samples/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>BraTS 2019</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>433 samples/sec</td><td>0.12 samples/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>BraTS 2019</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>202 samples/sec</td><td>0.16 samples/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>BraTS 2019</td><td>A100 SXM-80GB</td></tr><tr><td>RNN-T</td><td>82,540 samples/sec</td><td>36.23 samples/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>LibriSpeech</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>93,803 samples/sec</td><td>26.39 samples/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>47,255 samples/sec</td><td>36.16 samples/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td>BERT</td><td>17,697 samples/sec</td><td>7.73 samples/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>SQuAD v1.1</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>23,406 samples/sec</td><td>6.77 samples/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>9,865 samples/sec</td><td>7.76 samples/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td>DLRM</td><td>1,577,960 samples/sec</td><td>730.76 samples/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>Criteo 1TB Click Logs</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>2,115,950 samples/sec</td><td>619.54 samples/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>974,571 samples/sec</td><td>762.64 samples/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr></tbody></table></div> </small><h4>Power Efficiency Server Scenario - Closed Division</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Throughput</th><th>Throughput per Watt</th><th>GPU</th><th>Server</th><th>Dataset</th><th>GPU Version</th></tr></thead><tbody><tr><td>ResNet-50 v1.5</td><td>184,984 queries/sec</td><td>82.39 queries/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>ImageNet</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>239,991 queries/sec</td><td>69.53 queries/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>106,988 queries/sec</td><td>84.51 queries/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>ImageNet</td><td>A100 SXM-80GB</td></tr><tr><td>SSD ResNet-34</td><td>5,702 queries/sec</td><td>2.52 queries/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>COCO</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>6,301 queries/sec</td><td>1.82 queries/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>3,081 queries/sec</td><td>2.43 queries/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>COCO</td><td>A100 SXM-80GB</td></tr><tr><td>RNN-T</td><td>74,974 queries/sec</td><td>32.25 queries/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>LibriSpeech</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>87,984 queries/sec</td><td>24.78 queries/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>43,389 queries/sec</td><td>33.03 queries/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>LibriSpeech</td><td>A100 SXM-80GB</td></tr><tr><td>BERT</td><td>17,499 queries/sec</td><td>7.58 queries/sec/watt</td><td>8x A100</td><td>Gigabyte G482-Z54</td><td>SQuAD v1.1</td><td>A100 PCIe-40GB</td></tr><tr><td></td><td>21,492 queries/sec</td><td>6.03 queries/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>10,203 queries/sec</td><td>7.84 queries/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>SQuAD v1.1</td><td>A100 SXM-80GB</td></tr><tr><td>DLRM</td><td>2,001,940 queries/sec</td><td>575.72 queries/sec/watt</td><td>8x A100</td><td>DGX A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>890,334 queries/sec</td><td>663.62 queries/sec/watt</td><td>4x A100</td><td>DGX-Station-A100</td><td>Criteo 1TB Click Logs</td><td>A100 SXM-80GB</td></tr></tbody></table></div> </small><p class="chart-footnote"> MLPerf™ v1.0 Inference Closed: ResNet-50 v1.5, SSD ResNet-34, RNN-T, BERT 99% of FP32 accuracy target, 3D U-Net, DLRM 99.9% of FP32 accuracy target: 1.0-25, 1.0-26, 1.0-29, 1.0-30, 1.0-32, 1.0-55, 1.0-57. MLPerf name and logo are trademarks. See <a href="https://mlcommons.org/" target="_blank">https://mlcommons.org/</a> for more information.<br> BERT-Large sequence length = 384.<br> DLRM samples refers to 270 pairs/sample average<br> A10 and A30 results are preview<br> For MLPerf™ various scenario data, click <a href="https://github.com/mlcommons/inference_policies/blob/master/inference_rules.adoc#scenarios" target="_blank">here</a><br> For MLPerf™ latency constraints, click <a href="https://mlperf.org/inference-overview" target="_blank">here</a></p><div id="deeplearningperformance_inference_triton"></div><hr><div id="triton-benchmarks"></div> <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.4/Chart.min.js"></script><script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@0.7.0"></script><h3 style="color: #76b900;">NVIDIA TRITON Inference Server Delivered Comparable Performance to Custom Harness in MLPerf v1.0</h3> <br><p> NVIDIA landed top performance spots on all <a href="https://developer.nvidia.com/blog/extending-nvidia-performance-leadership-with-mlperf-inference-1-0-results/" target="_blank">MLPerf™ Inference 1.0 tests</a>, the AI-industry’s leading benchmark competition. For inference submissions, we have typically used a custom A100 inference serving harness. This custom harness has been designed and optimized specifically for providing the highest possible inference performance for MLPerf™ workloads, which require running inference on bare metal.</p> <canvas id="tritonmlperf" style="display: block; height: 0px; width: 0px;" height="0" width="0" class="chartjs-render-monitor"></canvas><script>
var ctx = document.getElementById('tritonmlperf').getContext('2d');
var myChart = new Chart(ctx, {
type: 'bar',
data: {
labels: ['Average Performance', 'Recommendation: DLRM', 'Image Classification: ResNet-50', 'NLP: BERT', 'Medical Imaging: 3D U-Net', 'Object Detection: SSD-Large'],
datasets: [{
datalabels: {
display:true,
align: 'end',
anchor: 'end'
},
barThickness: 70,
maxBarThickness: 80,
data: [97, 88, 99, 99, 100, 100],
backgroundColor: [
'rgba(118, 185, 0, 0.7)',
'rgba(118, 185, 0, 1)',
'rgba(118, 185, 0, 1)',
'rgba(118, 185, 0, 1)',
'rgba(118, 185, 0, 1)',
'rgba(118, 185, 0, 1)',
]
}]
},
options: {
animation: {
duration: 0
},
tooltips: {
enabled: false
},
hover: {
mode: null
},
responsive: true,
legend: {
display:false
},
scales: {
xAxes: [{
gridLines: {
display: false
}
}],
yAxes: [{
scaleLabel: {
display: true,
labelString: 'Triton / Custom Inference Serving'
},
gridLines: {
display:true
},
ticks: {
min: 0,
max: 105,
stepSize: 20,
suggestedMin: 0.5,
suggestedMax: 120.5,
fontSize: 14,
callback: function(label, index, labels){
switch (label) {
case 0:
return '0%';
case 20:
return '20%';
case 40:
return '40%';
case 60:
return '60%';
case 80:
return '80%';
case 100:
return '100%';
}
}
}
}]
}
}
});
</script><p class="chart-footnote">MLPerf™ v1.0 A100 Inference Closed: ResNet-50 v1.5, SSD ResNet-34, RNN-T, BERT 99% of FP32 accuracy target, 3D U-Net, DLRM 99% of FP32 accuracy target: 1.0-30, 1.0-31. MLPerf name and logo are trademarks.&nbsp;See&nbsp;<a href="https://mlcommons.org/en/" target="_blank">www.mlcommons.org</a>&nbsp;for more information.&ZeroWidthSpace;</p><p> The chart compares the performance of Triton to the custom MLPerf™ serving harness across five different TensorRT networks on A100 SXM-80GB on bare metal. The results show that Triton is highly efficient and delivers nearly equal or identical performance to the highly optimized MLPerf™ harness.</p><p>&nbsp;</p> <a name="cnn-inf-perf-triton"></a><h3 style="color: #76b900;">NVIDIA Client Batch Size=1 Performance with Triton Inference Server</h3><h4></h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Accelerator</th><th>Training Framework</th><th>Framework Backend</th><th>Precision</th><th>Model Instances on Triton</th><th>Client Batch Size</th><th>Dynamic Batch Size (Triton)</th><th>Number of Concurrent Client Requests</th><th>Latency (ms)</th><th>Throughput</th><th>Sequence/Input Length</th><th>Triton Container Version</th></tr></thead><tbody><tr><td>ResNet-50 V1.5 Inference</td><td>A100-PCIE-40GB</td><td>PyTorch</td><td>TensorRT</td><td>FP16</td><td>2</td><td>1</td><td>64</td><td>256</td><td>61.02</td><td>4,197 inf/sec</td><td>-</td><td>20.07-py3</td></tr><tr><td>ResNet-50 V1.5 Inference</td><td>A100-SXM4-40GB</td><td>PyTorch</td><td>TensorRT</td><td>TF32</td><td>2</td><td>1</td><td>64</td><td>256</td><td>48.35</td><td>5,294 inf/sec</td><td>-</td><td>21.03-py3</td></tr><tr><td>ResNet-50 V1.5 Inference</td><td>NVIDIA T4</td><td>PyTorch</td><td>TensorRT</td><td>FP16</td><td>1</td><td>1</td><td>64</td><td>256</td><td>257.91</td><td>992 inf/sec</td><td>-</td><td>20.07-py3</td></tr><tr><td>ResNet-50 V1.5 Inference</td><td>V100 SXM2-32GB</td><td>PyTorch</td><td>TensorRT</td><td>FP32</td><td>4</td><td>1</td><td>64</td><td>384</td><td>215.79</td><td>1,781 inf/sec</td><td>-</td><td>21.03-py3</td></tr><tr><td>BERT Large Inference</td><td>A100-PCIE-40GB</td><td>TensorFlow</td><td>TensorRT</td><td>FP16</td><td>1</td><td>1</td><td>8</td><td>16</td><td>17.48</td><td>915 inf/sec</td><td>384</td><td>20.09-py3</td></tr><tr><td>BERT Large Inference</td><td>A100-SXM4-40GB</td><td>TensorFlow</td><td>TensorRT</td><td>INT8</td><td>2</td><td>1</td><td>8</td><td>64</td><td>56.34</td><td>1,136 inf/sec</td><td>384</td><td>20.09-py3</td></tr><tr><td>BERT Large Inference</td><td>NVIDIA T4</td><td>TensorFlow</td><td>TensorRT</td><td>FP16</td><td>1</td><td>1</td><td>8</td><td>16</td><td>81.14</td><td>197 inf/sec</td><td>384</td><td>20.09-py3</td></tr><tr><td>DLRM Inference</td><td>A100-PCIE-40GB</td><td>PyTorch</td><td>Torchscript</td><td>Mixed</td><td>2</td><td>1</td><td>65,536</td><td>24</td><td>2.52</td><td>9,521 inf/sec</td><td>-</td><td>21.05-py3</td></tr><tr><td>DLRM Inference</td><td>A100-SXM4-40GB</td><td>PyTorch</td><td>Torchscript</td><td>FP16</td><td>2</td><td>1</td><td>65,536</td><td>30</td><td>2.71</td><td>11,076 inf/sec</td><td>-</td><td>21.03-py3</td></tr><tr><td>DLRM Inference</td><td>V100-SXM2-32GB</td><td>PyTorch</td><td>Torchscript</td><td>Mixed</td><td>2</td><td>1</td><td>65,536</td><td>30</td><td>4.24</td><td>7,068 inf/sec</td><td>-</td><td>21.05-py3</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><hr><div id="ngc-inference"></div><h3 style="color: #76b900;">Inference Performance of NVIDIA A100, A40, A30, A10, V100 and T4</h3><p class="chart-footnote">Benchmarks are reproducible by following links to NGC scripts</p><div id="resnet50-time-to-solution-v100"></div><div class="chart-container"><h4 class="chart-title">Inference Natural Langugage Processing</h4><p class="chart-subtitle">BERT Inference Throughput</p><div class="legend"></div> <svg id="bert-throughput-a100" data-nvidia-chart="true" data-is-grouped="" data-chart-legend="" data-bar-padding="5" data-chart-bars="[{&quot;axisLabel&quot;:&quot;NVIDIA A100 with Sparsity&quot;,&quot;barLabel&quot;:&quot;6,188&quot;,&quot;value&quot;:6188,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;NVIDIA V100&quot;,&quot;barLabel&quot;:&quot;897&quot;,&quot;value&quot;:897,&quot;class&quot;:&quot;green&quot;}]" data-chart-ticks="[{&quot;value&quot;:0,&quot;tickLabel&quot;:&quot;0&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:1000,&quot;tickLabel&quot;:&quot;1,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:2000,&quot;tickLabel&quot;:&quot;2,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:3000,&quot;tickLabel&quot;:&quot;3,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:4000,&quot;tickLabel&quot;:&quot;4,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:5000,&quot;tickLabel&quot;:&quot;5,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:6000,&quot;tickLabel&quot;:&quot;6,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:7000,&quot;tickLabel&quot;:&quot;7,000&quot;,&quot;class&quot;:&quot;&quot;}]" data-x-axis-label="Sequences Per Second"> </svg><p class="chart-footnote"> DGX A100 server w/ 1x NVIDIA A100 with 7 MIG instances of 1g.5gb | Batch Size = 94 | Precision: INT8 | Sequence Length = 128 <br>DGX-1 server w/ 1x NVIDIA V100 | TensorRT 7.1 | Batch Size = 256 | Precision: Mixed | Sequence Length = 128</p></div> <a name="cnn-inf-perf-net-a100"></a><p>&nbsp;</p><h4 class="chart-title"></h4><h3 style="color: #76b900;"></h3><h4>NVIDIA A100 BERT Inference Benchmarks</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Network<br>Type</th><th>Batch<br>Size</th><th>Throughput</th><th>Efficiency</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td>BERT-Large with Sparsity</td><td>Attention</td><td>94</td><td>6,188 sequences/sec</td><td>-</td><td>-</td><td>1x A100</td><td>DGX A100</td><td>-</td><td>INT8</td><td>SQuaD v1.1</td><td>-</td><td>A100 SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote"> A100 with 7 MIG instances of 1g.5gb | Sequence length=128 | Efficiency based on board power<br> Containers with a hyphen indicates a pre-release container</p><div id="resnet50-time-to-solution-v100"></div><div class="chart-container"><h4 class="chart-title">Inference Image Classification on CNNs with TensorRT</h4><p class="chart-subtitle">ResNet-50 v1.5 Throughput</p><div class="legend"></div> <svg id="resnet50-throughput" data-nvidia-chart="true" data-is-grouped="" data-chart-legend="" data-bar-padding="5" data-chart-bars="[{&quot;axisLabel&quot;:&quot;NVIDIA A100&quot;,&quot;barLabel&quot;:&quot;28,222&quot;,&quot;value&quot;:28222,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;A40&quot;,&quot;barLabel&quot;:&quot;15,510&quot;,&quot;value&quot;:15510,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;A30&quot;,&quot;barLabel&quot;:&quot;14,568&quot;,&quot;value&quot;:14568,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;A10&quot;,&quot;barLabel&quot;:&quot;10,928&quot;,&quot;value&quot;:10928,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;NVIDIA V100&quot;,&quot;barLabel&quot;:&quot;7,857&quot;,&quot;value&quot;:7857,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;NVIDIA T4&quot;,&quot;barLabel&quot;:&quot;4,960&quot;,&quot;value&quot;:4960,&quot;class&quot;:&quot;green&quot;}]" data-chart-ticks="[{&quot;value&quot;:0,&quot;tickLabel&quot;:&quot;0&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:5000,&quot;tickLabel&quot;:&quot;5,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:10000,&quot;tickLabel&quot;:&quot;10,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:15000,&quot;tickLabel&quot;:&quot;15,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:20000,&quot;tickLabel&quot;:&quot;20,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:25000,&quot;tickLabel&quot;:&quot;25,000&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:30000,&quot;tickLabel&quot;:&quot;30,000&quot;,&quot;class&quot;:&quot;&quot;}]" data-x-axis-label="Images Per Second"> </svg><p class="chart-footnote"> DGX A100: EPYC 7742@2.25GHz w/ 1x NVIDIA A100-SXM-80GB | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>GIGABYTE G482-Z52-00: EPYC@2.25GHz w/ 1x NVIDIA A40 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>GIGABYTE G482-Z52-SW-QZ-001: EPYC 7742@2.25GHz w/ 1x NVIDIA A30 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>GIGABYTE G482-Z52-00: EPYC@2.25GHz w/ 1x NVIDIA A10 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>DGX-2: Platinum 8168 @2.7GHz w/ 1x NVIDIA V100-SXM3-32GB | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: Mixed | Dataset: Synthetic<br>Supermicro SYS-4029GP-TRT: Xeon Gold 6240 @2.6 GHz w/ 1x NVIDIA T4 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic</p></div><p> &nbsp;<br> &nbsp;<br></p><div id="resnet50-time-to-solution-v100"></div><div class="chart-container"><p class="chart-subtitle">ResNet-50 v1.5 Power Efficiency</p><div class="legend"></div> <svg id="resnet50-power-efficiency" data-nvidia-chart="true" data-is-grouped="" data-chart-legend="" data-bar-padding="5" data-chart-bars="[{&quot;axisLabel&quot;:&quot;NVIDIA A100&quot;,&quot;barLabel&quot;:&quot;76&quot;,&quot;value&quot;:76,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;A40&quot;,&quot;barLabel&quot;:&quot;52&quot;,&quot;value&quot;:52,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;A30&quot;,&quot;barLabel&quot;:&quot;88&quot;,&quot;value&quot;:88,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;A10&quot;,&quot;barLabel&quot;:&quot;73&quot;,&quot;value&quot;:72,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;NVIDIA V100&quot;,&quot;barLabel&quot;:&quot;23&quot;,&quot;value&quot;:23,&quot;class&quot;:&quot;green&quot;},{&quot;axisLabel&quot;:&quot;NVIDIA T4&quot;,&quot;barLabel&quot;:&quot;71&quot;,&quot;value&quot;:71,&quot;class&quot;:&quot;green&quot;}]" data-chart-ticks="[{&quot;value&quot;:0,&quot;tickLabel&quot;:&quot;0&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:20,&quot;tickLabel&quot;:&quot;20&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:40,&quot;tickLabel&quot;:&quot;40&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:60,&quot;tickLabel&quot;:&quot;60&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:80,&quot;tickLabel&quot;:&quot;80&quot;,&quot;class&quot;:&quot;&quot;},{&quot;value&quot;:100,&quot;tickLabel&quot;:&quot;100&quot;,&quot;class&quot;:&quot;&quot;}]" data-x-axis-label="Images Per Second Per Watt"> </svg><p class="chart-footnote"> DGX A100: EPYC 7742@2.25GHz w/ 1x NVIDIA A100-SXM-80GB | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>GIGABYTE G482-Z52-00: EPYC@2.25GHz w/ 1x NVIDIA A40 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>GIGABYTE G482-Z52-SW-QZ-001: EPYC 7742@2.25GHz w/ 1x NVIDIA A30 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>GIGABYTE G482-Z52-00: EPYC@2.25GHz w/ 1x NVIDIA A10 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic<br>DGX-2: Platinum 8168 @2.7GHz w/ 1x NVIDIA V100-SXM3-32GB | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: Mixed | Dataset: Synthetic<br>Supermicro SYS-4029GP-TRT: Xeon Gold 6240 @2.6 GHz w/ 1x NVIDIA T4 | TensorRT 7.2 | Batch Size = 128 | 21.05-py3 | Precision: INT8 | Dataset: Synthetic</p></div> <a name="cnn-inf-perf-net"></a><p>&nbsp;</p><h3 style="color: #76b900;"></h3><h4>A100 1/7 MIG Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>1/7 MIG Throughput</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>1</td><td>1,445 images/sec</td><td>0.69</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>2,250 images/sec</td><td>0.89</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>3,508 images/sec</td><td>2.28</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>28</td><td>4,035 images/sec</td><td>6.94</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>4,492 images/sec</td><td>28.5</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>1</td><td>1,406 images/sec</td><td>0.71</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>2,170 images/sec</td><td>0.92</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>3,375 images/sec</td><td>2.37</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>27</td><td>3,956 images/sec</td><td>6.83</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>4,333 images/sec</td><td>29.54</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>1</td><td>800 sequences/sec</td><td>1.25</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>1,192 sequences/sec</td><td>1.68</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>4</td><td>474 sequences/sec</td><td>8.55</td><td>1x A100</td><td>DGX A100</td><td>20.11-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>1,706 sequences/sec</td><td>4.69</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>2,175 sequences/sec</td><td>58.85</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>1</td><td>268 sequences/sec</td><td>3.73</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>386 sequences/sec</td><td>5.18</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>5</td><td>517 sequences/sec</td><td>9.84</td><td>1x A100</td><td>DGX A100</td><td>20.11-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>558 sequences/sec</td><td>14.33</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>677 sequences/sec</td><td>189.2</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr></tbody></table></div> </small><p class="chart-footnote"> Containers with a hyphen indicates a pre-release container | Servers with a hyphen indicates a pre-production server<br> BERT-Large: Sequence Length = 128</p><h4>A100 7 MIG Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>7 MIG Throughput</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>1</td><td>10,017 images/sec</td><td>0.7</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>15,345 images/sec</td><td>0.91</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>24,035 images/sec</td><td>2.33</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>27</td><td>28,369 images/sec</td><td>6.67</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>31,301 images/sec</td><td>28.63</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>1</td><td>9,875 images/sec</td><td>0.71</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>15,159 images/sec</td><td>0.94</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>23,306 images/sec</td><td>2.4</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>27</td><td>27,498 images/sec</td><td>6.87</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>30,179 images/sec</td><td>29.73</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>1</td><td>5,595 sequences/sec</td><td>1.27</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>8,298 sequences/sec</td><td>1.69</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>4</td><td>3,284 sequences/sec</td><td>8.55</td><td>1x A100</td><td>DGX A100</td><td>20.11-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>11,878 sequences/sec</td><td>4.73</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>14,393 sequences/sec</td><td>62.37</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>1</td><td>1,880 sequences/sec</td><td>3.76</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>2</td><td>2,694 sequences/sec</td><td>5.21</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>5</td><td>3,575 sequences/sec</td><td>9.84</td><td>1x A100</td><td>DGX A100</td><td>20.11-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>8</td><td>3,838 sequences/sec</td><td>14.63</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM-80GB</td></tr><tr><td></td><td>128</td><td>4,467 sequences/sec</td><td>200.76</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100 SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote"> Containers with a hyphen indicates a pre-release container | Servers with a hyphen indicates a pre-production server<br> BERT-Large: Sequence Length = 128</p><h4>A100 Full Chip Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>Full Chip Throughput</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>2</td><td>3,995 images/sec</td><td>0.5</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>8</td><td>11,418 images/sec</td><td>0.7</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>128</td><td>29,109 images/sec</td><td>4.4</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>211</td><td>31,257 images/sec</td><td>6.75</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>2</td><td>4,020 images/sec</td><td>0.5</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>8</td><td>11,095 images/sec</td><td>0.72</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>128</td><td>28,222 images/sec</td><td>4.54</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>208</td><td>30,039 images/sec</td><td>6.92</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnext101_32x4d_pyt_amp">ResNext101</a></td><td>32</td><td>7,674 samples/sec</td><td>4.17</td><td>1x A100</td><td>-</td><td>-</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:efficientnet_b0_pyt_qat_ckpt_fp32">EfficientNet-B0</a></td><td>128</td><td>22,346 images/sec</td><td>5.73</td><td>1x A100</td><td>-</td><td>-</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>2</td><td>2,599 sequences/sec</td><td>0.77</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>8</td><td>6,889 sequences/sec</td><td>1.16</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>128</td><td>13,661 sequences/sec</td><td>9.37</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>256</td><td>14,490 sequences/sec</td><td>17.67</td><td>1x A100</td><td>DGX A100</td><td>-</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>2</td><td>1,093 sequences/sec</td><td>1.83</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM4-40GB</td></tr><tr><td></td><td>8</td><td>2,311 sequences/sec</td><td>3.46</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>128</td><td>4,515 sequences/sec</td><td>28.35</td><td>1x A100</td><td>DGX A100</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM-80GB</td></tr><tr><td></td><td>256</td><td>4,679 sequences/sec</td><td>54.71</td><td>1x A100</td><td>-</td><td>-</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A100-SXM4-80GB</td></tr></tbody></table></div> </small><p class="chart-footnote"> Containers with a hyphen indicates a pre-release container | Servers with a hyphen indicates a pre-production server<br> BERT-Large: Sequence Length = 128<br> For BS=1 inference refer to the Triton Inference Server section</p> <a name="cnn-inf-perf-net-a40"></a><p>&nbsp;</p><h4>A40 Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>Throughput</th><th>Efficiency</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>2</td><td>4,523 images/sec</td><td>24 images/sec/watt</td><td>0.44</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>8</td><td>9,279 images/sec</td><td>40 images/sec/watt</td><td>0.86</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>109</td><td>16,738 images/sec</td><td>- images/sec/watt</td><td>6.51</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>128</td><td>16,286 images/sec</td><td>54 images/sec/watt</td><td>7.86</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>2</td><td>4,451 images/sec</td><td>24 images/sec/watt</td><td>0.45</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>8</td><td>8,903 images/sec</td><td>38 images/sec/watt</td><td>0.9</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>109</td><td>15,892 images/sec</td><td>- images/sec/watt</td><td>6.86</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>128</td><td>15,510 images/sec</td><td>52 images/sec/watt</td><td>8.25</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>2</td><td>2,351 sequences/sec</td><td>13 sequences/sec/watt</td><td>0.85</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>8</td><td>4,494 sequences/sec</td><td>19 sequences/sec/watt</td><td>1.78</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>128</td><td>7,180 sequences/sec</td><td>27 sequences/sec/watt</td><td>17.83</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>2</td><td>893 sequences/sec</td><td>4 sequences/sec/watt</td><td>2.24</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>8</td><td>1,666 sequences/sec</td><td>6 sequences/sec/watt</td><td>4.8</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A40</td></tr><tr><td></td><td>128</td><td>2,216 sequences/sec</td><td>9 sequences/sec/watt</td><td>57.76</td><td>1x A40</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A40</td></tr></tbody></table></div> </small><p class="chart-footnote"><a href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt" target="blank">NGC: TensorRT Container</a><br> Sequence length=128 for BERT-BASE and BERT-LARGE | Efficiency based on board power<br> Containers with a hyphen indicates a pre-release container</p> <a name="cnn-inf-perf-net-a30"></a><p>&nbsp;</p><h4>A30 Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>Throughput</th><th>Efficiency</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>2</td><td>3,487 images/sec</td><td>38 images/sec/watt</td><td>0.57</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.03-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>8</td><td>8,497 images/sec</td><td>70 images/sec/watt</td><td>0.94</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.03-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>100</td><td>14,963 images/sec</td><td>- images/sec/watt</td><td>6.68</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>128</td><td>15,246 images/sec</td><td>93 images/sec/watt</td><td>8.4</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>2</td><td>3,498 images/sec</td><td>37 images/sec/watt</td><td>0.57</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.03-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>8</td><td>8,330 images/sec</td><td>68 images/sec/watt</td><td>0.96</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.03-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>95</td><td>14,191 images/sec</td><td>- images/sec/watt</td><td>6.69</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>128</td><td>14,568 images/sec</td><td>88 images/sec/watt</td><td>8.79</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>2</td><td>2,132 sequences/sec</td><td>22 sequences/sec/watt</td><td>0.94</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>8</td><td>4,515 sequences/sec</td><td>34 sequences/sec/watt</td><td>1.77</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>128</td><td>6,940 sequences/sec</td><td>48 sequences/sec/watt</td><td>18.44</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>2</td><td>837 sequences/sec</td><td>7 sequences/sec/watt</td><td>2.39</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>8</td><td>1,441 sequences/sec</td><td>12 sequences/sec/watt</td><td>5.55</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A30</td></tr><tr><td></td><td>128</td><td>2,211 sequences/sec</td><td>15 sequences/sec/watt</td><td>57.9</td><td>1x A30</td><td>GIGABYTE G482-Z52-SW-QZ-001</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A30</td></tr></tbody></table></div> </small><p class="chart-footnote"><a href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt" target="blank">NGC: TensorRT Container</a><br> Sequence length=128 for BERT-BASE and BERT-LARGE | Efficiency based on board power<br> Containers with a hyphen indicates a pre-release container</p> <a name="cnn-inf-perf-net-a10"></a><p>&nbsp;</p><h4>A10 Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>Throughput</th><th>Efficiency</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>2</td><td>4,282 images/sec</td><td>29 images/sec/watt</td><td>0.47</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>8</td><td>7,778 images/sec</td><td>52 images/sec/watt</td><td>1.03</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>75</td><td>10,846 images/sec</td><td>- images/sec/watt</td><td>6.82</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>128</td><td>11,520 images/sec</td><td>77 images/sec/watt</td><td>11.11</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>2</td><td>4,187 images/sec</td><td>28 images/sec/watt</td><td>0.48</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>8</td><td>7,434 images/sec</td><td>50 images/sec/watt</td><td>1.08</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>70</td><td>10,785 images/sec</td><td>- images/sec/watt</td><td>6.49</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>128</td><td>10,928 images/sec</td><td>73 images/sec/watt</td><td>11.71</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>2</td><td>2,095 sequences/sec</td><td>17 sequences/sec/watt</td><td>0.95</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>8</td><td>3,598 sequences/sec</td><td>26 sequences/sec/watt</td><td>2.22</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>128</td><td>4,764 sequences/sec</td><td>35 sequences/sec/watt</td><td>26.87</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>2</td><td>779 sequences/sec</td><td>6 sequences/sec/watt</td><td>2.57</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>8</td><td>1,288 sequences/sec</td><td>10 sequences/sec/watt</td><td>6.21</td><td>1x A10</td><td>GIGABYTE G482-Z52-00</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A10</td></tr><tr><td></td><td>128</td><td>1,368 sequences/sec</td><td>10 sequences/sec/watt</td><td>93.57</td><td>1x A10</td><td>Supermicro SYS-1029GQ-TRT</td><td>20.11-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>A10</td></tr></tbody></table></div> </small><p class="chart-footnote"><a href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt" target="blank">NGC: TensorRT Container</a><br> Sequence length=128 for BERT-BASE and BERT-LARGE | Efficiency based on board power<br> Containers with a hyphen indicates a pre-release container</p> <a name="cnn-inf-perf-net-v100"></a><p>&nbsp;</p><h4>V100 Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>Throughput</th><th>Efficiency</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>2</td><td>1,995 images/sec</td><td>11 images/sec/watt</td><td>1</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Synthetic</td><td>TensorRT 7.2.3</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>8</td><td>4,305 images/sec</td><td>17 images/sec/watt</td><td>1.86</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Synthetic</td><td>TensorRT 7.2.3</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>52</td><td>7,909 images/sec</td><td>- images/sec/watt</td><td>6.57</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2.3</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>128</td><td>8,202 images/sec</td><td>24 images/sec/watt</td><td>15.61</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Synthetic</td><td>TensorRT 7.2.3</td><td>V100-SXM3-32GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>2</td><td>2,005 images/sec</td><td>11 images/sec/watt</td><td>1</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Synthetic</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>8</td><td>4,221 images/sec</td><td>16 images/sec/watt</td><td>1.9</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>52</td><td>7,545 images/sec</td><td>- images/sec/watt</td><td>6.89</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Synthetic</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>128</td><td>7,857 images/sec</td><td>23 images/sec/watt</td><td>16.29</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>2</td><td>1,219 sequences/sec</td><td>6 sequences/sec/watt</td><td>1.64</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Sample Text</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>8</td><td>2,300 sequences/sec</td><td>8 sequences/sec/watt</td><td>3.48</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Sample Text</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>128</td><td>3,206 sequences/sec</td><td>10 sequences/sec/watt</td><td>39.93</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Sample Text</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>2</td><td>530 sequences/sec</td><td>2 sequences/sec/watt</td><td>3.78</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>Mixed</td><td>Sample Text</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>8</td><td>793 sequences/sec</td><td>3 sequences/sec/watt</td><td>10.09</td><td>1x V100</td><td>DGX-2</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr><tr><td></td><td>128</td><td>978 sequences/sec</td><td>3 sequences/sec/watt</td><td>130.92</td><td>1x V100</td><td>DGX-2</td><td>20.11-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>V100-SXM3-32GB</td></tr></tbody></table></div> </small><p class="chart-footnote"><a href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt" target="blank">NGC: TensorRT Container</a><br> Sequence length=128 for BERT-BASE and BERT-LARGE | Efficiency based on board power<br> Containers with a hyphen indicates a pre-release container<br> For BS=1 inference refer to the Triton Inference Server section</p> <a name="cnn-inf-perf-net-t4"></a><p>&nbsp;</p><h4>T4 Inference Performance</h4> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Network</th><th>Batch Size</th><th>Throughput</th><th>Efficiency</th><th>Latency (ms)</th><th>GPU</th><th>Server</th><th>Container</th><th>Precision</th><th>Dataset</th><th>Framework</th><th>GPU Version</th></tr></thead><tbody><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:resnet50_pyt_amp">ResNet-50</a></td><td>2</td><td>2,105 images/sec</td><td>30 images/sec/watt</td><td>0.95</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>21.03-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>8</td><td>3,846 images/sec</td><td>55 images/sec/watt</td><td>2.08</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>32</td><td>4,798 images/sec</td><td>- images/sec/watt</td><td>6.67</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>128</td><td>5,358 images/sec</td><td>77 images/sec/watt</td><td>23.89</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:rn50_tf_amp_ckpt">ResNet-50v1.5</a></td><td>2</td><td>2,092 images/sec</td><td>30 images/sec/watt</td><td>0.96</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>21.03-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>8</td><td>3,698 images/sec</td><td>53 images/sec/watt</td><td>2.16</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>29</td><td>4,540 images/sec</td><td>- images/sec/watt</td><td>6.39</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>128</td><td>4,960 images/sec</td><td>71 images/sec/watt</td><td>25.81</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Synthetic</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_base_qa_squad11_amp_128">BERT-BASE</a></td><td>2</td><td>1,089 sequences/sec</td><td>18 sequences/sec/watt</td><td>1.84</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>8</td><td>1,677 sequences/sec</td><td>25 sequences/sec/watt</td><td>4.77</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>128</td><td>1,818 sequences/sec</td><td>28 sequences/sec/watt</td><td>70.4</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>20.11-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td><a href="https://ngc.nvidia.com/catalog/models/nvidia:bert_tf_ckpt_large_qa_squad11_amp_128">BERT-LARGE</a></td><td>2</td><td>386 sequences/sec</td><td>6 sequences/sec/watt</td><td>5.18</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>8</td><td>551 sequences/sec</td><td>9 sequences/sec/watt</td><td>14.52</td><td>1x T4</td><td>Supermicro SYS-4029GP-TRT</td><td>21.05-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr><tr><td></td><td>128</td><td>561 sequences/sec</td><td>8 sequences/sec/watt</td><td>227.98</td><td>1x T4</td><td>Supermicro SYS-1029GQ-TRT</td><td>20.11-py3</td><td>INT8</td><td>Sample Text</td><td>TensorRT 7.2</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote"><a href="https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt" target="blank">NGC: TensorRT Container</a><br> Sequence length=128 for BERT-BASE and BERT-LARGE | Efficiency based on board power<br> Containers with a hyphen indicates a pre-release container<br> For BS=1 inference refer to the Triton Inference Server section</p></div><div class="tab-pane" id="dl-pipeline"> <a name="cnn-pipeline-perf"></a><div id="deeplearningperformance_pipeline"></div><p> NVIDIA Jarvis is an application framework for multimodal conversational AI services that delivers real-time performance on GPUs. Jarvis 1.0 Beta includes fully optimized pipelines for Automatic Speech Recognition (ASR), Natural Language Processing (NLP), Text to Speech (TTS) that can be used for deploying real-time conversational AI apps such as transcription, virtual assistants and chatbots. Please visit <a href="https://developer.nvidia.com/nvidia-jarvis-getting-started" target="_blank">Jarvis – Getting Started</a> to download and get started with Jarvis.</p> <br><div id="pipeline-benchmarks"></div><h3 style="color: #76b900;">Jarvis Benchmarks</h3><h4>Automatic Speech Recognition</h4><h5>A100 Best Streaming Throughput Mode (800 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>14.4</td><td>1</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>256</td><td>254.3</td><td>64</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>512</td><td>351.2</td><td>506</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>1024</td><td>630.8</td><td>1005</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>1</td><td>17.6</td><td>1</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>256</td><td>244.9</td><td>254</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>512</td><td>381</td><td>507</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>1024</td><td>749.3</td><td>1,004</td><td>A100 SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>A100 Best Streaming Latency Mode (100 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>9.6</td><td>1</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>16</td><td>25.9</td><td>16</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>128</td><td>132.4</td><td>128</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>1</td><td>13.4</td><td>1</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>16</td><td>26.3</td><td>16</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>128</td><td>258.9</td><td>128</td><td>A100 SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>A100 Offline Mode (3200 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>28.1</td><td>1</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>512</td><td>566.5</td><td>505</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>1,024</td><td>899.3</td><td>1,000</td><td>A100 SXM4-40GB</td></tr><tr><td>Quartznet</td><td>1,512</td><td>1,303.8</td><td>1,460</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>1</td><td>31</td><td>1</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>512</td><td>667.5</td><td>504</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>1,024</td><td>1,089</td><td>997</td><td>A100 SXM4-40GB</td></tr><tr><td>Jasper</td><td>1,512</td><td>1,753.8</td><td>1,449</td><td>A100 SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>V100 Best Streaming Throughput Mode (800 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>14.4</td><td>1</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>256</td><td>222.2</td><td>254</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>512</td><td>385.2</td><td>505</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>768</td><td>574.5</td><td>752</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>1</td><td>26.8</td><td>1</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>128</td><td>239.4</td><td>127</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>256</td><td>416</td><td>253</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>512</td><td>969.7</td><td>500</td><td>V100 SXM2-16GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>V100 Best Streaming Latency Mode (100 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>8.8</td><td>1</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>16</td><td>22.4</td><td>16</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>128</td><td>114.7</td><td>127</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>1</td><td>21.5</td><td>1</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>16</td><td>36.9</td><td>16</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>64</td><td>406.4</td><td>64</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>512</td><td>969.7</td><td>500</td><td>V100 SXM2-16GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>V100 Offline Mode (3200 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>32.933</td><td>1</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>256</td><td>461.44</td><td>253</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>512</td><td>784.73</td><td>502</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>768</td><td>1,121.6</td><td>747</td><td>V100 SXM2-16GB</td></tr><tr><td>Quartznet</td><td>1,024</td><td>1,551.5</td><td>986</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>1</td><td>48.351</td><td>1</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>256</td><td>734.99</td><td>252</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>512</td><td>1,423.3</td><td>498</td><td>V100 SXM2-16GB</td></tr><tr><td>Jasper</td><td>768</td><td>2,190.2</td><td>730</td><td>V100 SXM2-16GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>T4 Best Streaming Throughput Mode (800 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>33.183</td><td>1</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>64</td><td>162.63</td><td>64</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>128</td><td>263.6</td><td>127</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>256</td><td>449.28</td><td>253</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>384</td><td>732.75</td><td>376</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>1</td><td>72.377</td><td>1</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>64</td><td>259.64</td><td>64</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>128</td><td>450.81</td><td>127</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>256</td><td>1,200.8</td><td>249</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>T4 Best Streaming Latency Mode (100 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>19.2</td><td>1</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>16</td><td>56.4</td><td>16</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>64</td><td>242.4</td><td>64</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>1</td><td>46.9</td><td>1</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>8</td><td>51.1</td><td>8</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>16</td><td>84.4</td><td>16</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>T4 Offline Mode (3200 ms chunk)</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Acoustic model</th><th># of streams</th><th>Latency (ms) (avg)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>Quartznet</td><td>1</td><td>157.62</td><td>1</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>256</td><td>906.17</td><td>251</td><td>NVIDIA T4</td></tr><tr><td>Quartznet</td><td>512</td><td>1,515.2</td><td>495</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>1</td><td>96.201</td><td>1</td><td>NVIDIA T4</td></tr><tr><td>Jasper</td><td>256</td><td>1,758.4</td><td>247</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote">ASR Throughput (RTFX) - Number of seconds of audio processed per second | Audio Chunk Size – Server side configuration indicating the amount of new data to be considered by the acoustic model | ASR Dataset: Librispeech | The latency numbers were measured using the streaming recognition mode, with the BERT-Base punctuation model enabled, a 4-gram language model, a decoder beam width of 128 and timestamps enabled. The client and the server were using audio chunks of the same duration (100ms, 800ms, 3200ms depending on the server configuration). The Jarvis streaming client jarvis_streaming_asr_client, provided in the Jarvis client image was used with the --simulate_realtime flag to simulate transcription from a microphone, where each stream was doing 5 iterations over a sample audio file from the Librispeech dataset (1272-135031-0000.wav) | Jarvis version: v1.0.0-b1 | Hardware: NVIDIA DGX A100 (1x A100 SXM4-40GB), NVIDIA DGX-1 (1x V100-SXM2-16GB), NVIDIA T4 with 2x Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz</p><h4>Natural Language Processing</h4><h5>A100 Benchmarks</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Task</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (seq/sec)</th><th>GPU Version</th></tr></thead><tbody><tr><td>NER</td><td>1</td><td>3.19</td><td>311</td><td>A100 SXM4-40GB</td></tr><tr><td>NER</td><td>256</td><td>95.5</td><td>2549</td><td>A100 SXM4-40GB</td></tr><tr><td>Q&amp;A</td><td>1</td><td>4.95</td><td>201</td><td>A100 SXM4-40GB</td></tr><tr><td>Q&amp;A</td><td>128</td><td>279</td><td>453</td><td>A100 SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>V100 Benchmarks</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Task</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (seq/sec)</th><th>GPU Version</th></tr></thead><tbody><tr><td>NER</td><td>1</td><td>4.87</td><td>204</td><td>V100 SXM2-16GB</td></tr><tr><td>NER</td><td>256</td><td>135</td><td>1,797</td><td>V100 SXM2-16GB</td></tr><tr><td>Q&amp;A</td><td>1</td><td>7.47</td><td>134</td><td>V100 SXM2-16GB</td></tr><tr><td>Q&amp;A</td><td>128</td><td>521</td><td>244</td><td>V100 SXM2-16GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>T4 Benchmarks</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th>Task</th><th># of streams</th><th>Avg Latency (ms)</th><th>Throughput (seq/sec)</th><th>GPU Version</th></tr></thead><tbody><tr><td>NER</td><td>1</td><td>9.31</td><td>107</td><td>NVIDIA T4</td></tr><tr><td>NER</td><td>256</td><td>255</td><td>960</td><td>NVIDIA T4</td></tr><tr><td>Q&amp;A</td><td>1</td><td>11.5</td><td>87</td><td>NVIDIA T4</td></tr><tr><td>Q&amp;A</td><td>128</td><td>571</td><td>223</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote">Named Entity Recogniton (NER): 128 seq len, BERT-base | Question Answering (QA): 384 seq len, BERT-large | NLP Throughput (seq/s) - Number of sequences processed per second | Performance of the Jarvis named entity recognition (NER) service (using a BERT-base model, sequence length of 128) and the Jarvis question answering (QA) service (using a BERT-large model, sequence length of 384) was measured in Jarvis. Batch size 1 latency and maximum throughput were measured. Jarvis version: v1.0.0-b1 | Hardware: NVIDIA DGX A100 (1x A100 SXM4-40GB), NVIDIA DGX-1 (1x V100-SXM2-16GB), NVIDIA T4 with 2x Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz</p><h4>Text to Speech</h4><h5>A100 Benchmarks</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th># of streams</th><th>Avg Latency to first audio (sec)</th><th>Avg Latency between audio chunks (sec)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>1</td><td>0.06</td><td>0.04</td><td>20</td><td>A100 SXM4-40GB</td></tr><tr><td>4</td><td>0.48</td><td>0.03</td><td>37</td><td>A100 SXM4-40GB</td></tr><tr><td>6</td><td>0.69</td><td>0.03</td><td>42</td><td>A100 SXM4-40GB</td></tr><tr><td>8</td><td>0.88</td><td>0.03</td><td>46</td><td>A100 SXM4-40GB</td></tr><tr><td>10</td><td>1.06</td><td>0.03</td><td>49</td><td>A100 SXM4-40GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>V100 Benchmarks</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th># of streams</th><th>Avg Latency to first audio (sec)</th><th>Avg Latency between audio chunks (sec)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>1</td><td>0.08</td><td>0.05</td><td>14</td><td>V100 SXM2-16GB</td></tr><tr><td>4</td><td>0.77</td><td>0.05</td><td>23</td><td>V100 SXM2-16GB</td></tr><tr><td>6</td><td>1.11</td><td>0.05</td><td>26</td><td>V100 SXM2-16GB</td></tr><tr><td>8</td><td>1.4</td><td>0.06</td><td>28</td><td>V100 SXM2-16GB</td></tr><tr><td>10</td><td>1.74</td><td>0.07</td><td>28</td><td>V100 SXM2-16GB</td></tr></tbody></table></div> </small><p class="chart-footnote"></p><h5>T4 Benchmarks</h5> <small><div class="table-wrapper"><table class="table table-striped"><thead><tr><th># of streams</th><th>Avg Latency to first audio (sec)</th><th>Avg Latency between audio chunks (sec)</th><th>Throughput (RTFX)</th><th>GPU Version</th></tr></thead><tbody><tr><td>1</td><td>0.12</td><td>0.07</td><td>11</td><td>NVIDIA T4</td></tr><tr><td>4</td><td>1.02</td><td>0.07</td><td>17</td><td>NVIDIA T4</td></tr><tr><td>6</td><td>1.59</td><td>0.07</td><td>18</td><td>NVIDIA T4</td></tr><tr><td>8</td><td>2.13</td><td>0.08</td><td>19</td><td>NVIDIA T4</td></tr><tr><td>10</td><td>2.55</td><td>0.1</td><td>18</td><td>NVIDIA T4</td></tr></tbody></table></div> </small><p class="chart-footnote">TTS Throughput (RTFX) - Number of seconds of audio generated per second | Dataset: LJSpeech | Performance of the Jarvis text-to-speech (TTS) service was measured for different number of parallel streams. Each parallel stream performed 10 iterations over 10 input strings from the LJSpeech dataset. Latency to first audio chunk and latency between successive audio chunks and throughput were measured. Jarvis version: v1.0.0-b1 | Hardware: NVIDIA DGX A100 (1x A100 SXM4-40GB), NVIDIA DGX-1 (1x V100-SXM2-16GB), NVIDIA T4 with 2x Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz</p></div><p>&nbsp;</p><p class="chart-footnote">Last updated: June 30th, 2021</p><p>&nbsp;</p></div></div></div></div></div></section></div> </section>
		</div>
	</div>

	<div class="section section-projects">
		<div class="row content">   
			<div class="col-sm-4 area">
				<div class="title">
					<span>Privacy-Preserving</span> <span>Computing</span>
				</div>
				<div class="icon">
					<img src="images/4-3.svg">
				</div>
			</div>
			<div class="col-sm-8 image">
				<div class="project">
					<div class="name">Sphinx: Enabling Privacy-Preserving Online Learning over the Cloud</div> 
					<div class="authors">Authors: Han Tian, Chaoliang Zeng, Zhenghang Ren, Di Chai, Kai Chen, Qiang Yang</div> 
					<div class="abstract">We present Sphinx, a privacy-preserving online learning system that strikes a balance between model performance, computational efficiency, and privacy preservation. At its core, Sphinx combines homomorphic encryption and differential privacy reciprocally to maintain the model with most of its parameters as plaintexts, enabling fast training and inference protocol designs.</div> 
				</div>
				<div class="project">
					<div class="name">FedEval: A Benchmark System with a Comprehensive Evaluation Model for Federated Learning</div> 
					<div class="authors">Authors: Di Chai, Leye Wang, Junxue Zhang, Kai Chen, Qiang Yang</div> 
					<div class="abstract">We propose a comprehensive evaluation framework for FL systems. Specifically, we first introduce the PRACT model, which defines five metrics that cannot be excluded from FL evaluation, including Privacy, Robustness, Accuracy, Communication, and Time efficiency. Then we design and implement a benchmarking system called FedEval, which enables the systematic evaluation and comparison of existing works under consistent experimental conditions. </div> 
				</div>
			</div>  
		</div>
	</div>

	<div class="section section-projects">
		<div class="row content">   
			<div class="col-sm-4 area">
				<div class="title">
					<span>Network for</span> <span>Machine Learning</span>
				</div>
				<div class="icon">
					<img src="images/4-1.svg">
				</div>
			</div>
			<div class="col-sm-8 image">
				<div class="project">
					<div class="name">Domain-Specific Communication Optimization for Distributed DNN Training</div> 
					<div class="authors">Authors: Hao Wang, Jingrong Chen, Xinchen Wan, Han Tian, Jiacheng Xia, Gaoxiong Zeng, Kai Chen, Wei Bai, Junchen Jiang</div> 
					<div class="abstract">We present DLCP, a network substrate that speeds up DNN training by fully embracing several unique characteristics of deep learning. Compared to prior work in this space, DLCP integrates three simple-yet-effective techniques to form a multi-layered protection against long tail latency caused by transient packet drops and queueing. </div> 
				</div>
			</div>  
		</div>
	</div>

	<div class="section section-projects">
		<div class="row content">   
			<div class="col-sm-4 area">
				<div class="title">
					<span>AI Algorithm &</span> <span>Applications</span>
				</div>
				<div class="icon">
					<img src="images/4-2.svg">
				</div>
			</div>
			<div class="col-sm-8 image">
				<div class="project">
					<div class="name">CityNet: A Multi-city, Multi-modal Dataset for Smart City</div> 
					<div class="authors">Authors: Xu Geng, Yilun Jin, Zhengfei Zheng, Yu Yang, Yexin Li, Han Tian, Peibo Duan, Leye Wang, Jiannong Cao, Hai Yang, Qiang Yang, Kai Chen</div> 
					<div class="abstract">We present CityNet, a multi-modal urban dataset containing data from 7 cities, each of which coming from 3 data sources. To introduce CityNet, we first present its generation process as well as its basic properties. In addition, to facilitate the use of CityNet, we carry out extensive data mining and machine learning experiments, including spatio-temporal predictions, transfer learning, and reinforcement learning.</div> 
				</div>
			</div>  
		</div>
	</div>


	<div class="section section-content section-footer">
		<div class="row title">
			<div class="col">
				<div>an RGC Theme-based Research Scheme-funded Project</div>
			</div>
		</div>
	</div>


</div> 
</body>
</html>
